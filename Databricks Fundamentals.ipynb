{"cells":[{"cell_type":"markdown","source":["# Databricks hands-on Challenge\n\nThe goal of this exercise is to simulate a real data engineering + data science scenario in one of our customers. This will include:\n- obtaining a data set\n- process the data to clean it up\n- explore with visualizations and sql queries\n- extract to cosmos\n- use a machine learning algorithm to make generalizations and predictions\n\n\nThis exercise requires an Azure Subscription and the use of some Azure Services in adition to Azure Databricks.\n\n_You real goal, should you accept it, is to discover one of of the most important results in stellar astrophysics._"],"metadata":{}},{"cell_type":"markdown","source":["## Download and uncompress the data data\n\nGaia is an ESA satelite mapping light sources in the sky. A sample dataset of the Gaia Data Release 2 (GaiaRD2) is available here (size 12415259 bytes): http://cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/GaiaSource_1000172165251650944_1000424567594791808.csv.gz\n\nThe full Gaia DR2 dataset includes **61000** files like this for a total size of over 550 GB.\n\n**Hint**: use the %sh command to jump into the Linux shell, wget to get the file, and gzip to decompress the file.\n\n**Hint 2**: wget -O lets you specify a target folder (eg, /tmp/filename.csv.gz) and gzip -d -f can be used for decompressing."],"metadata":{}},{"cell_type":"code","source":["%sh\n\n# TODO: optionally, create a folder to hold the file (e.g., /mnt/gaiadr2)\n\n# TODO: download the file with wget\n\n# TODO: decompress the file\n\n# TODO: list the contents of the folder"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Create an Azure Storage Account in the Azure Portal\n\n1. Create an Azore Blog Storage account - See instructions here: https://docs.microsoft.com/en-gb/azure/storage/common/storage-quickstart-create-account\n\n2. Create a Container in the Storage account. See intructions here: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal\n\n3. Write down the name of the storage account, container name, and one of the access keys (https://docs.microsoft.com/en-us/azure/storage/common/storage-account-manage)\n\nNotes:\n- use the same region where the Databricks cluster was created\n- use Performance=Standard, Account Kind = Blob Storage, Replication = LRS and Access Tier = Hot"],"metadata":{}},{"cell_type":"markdown","source":["## Mount the Storage Account/Container in Databricks\n\nThis will make the storage account visible into Databricks' distributed file system (DBFS). Mounted folders are persistent accross the workspace, this only has to be done once.\n\nSe instructions here: check section \"Mount an Azure Blob Storage container\" here: https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html"],"metadata":{}},{"cell_type":"code","source":["try:\n    # TODO - MOUNT THE STORAGE\nexcept:\n    print(\"(Mount already existed, ignoring)\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# Move the file into the DBFS location you just mounted\n\n**Hint**: use dbutils.fs.mv(source, destination). List the contents of the target location using dbutils.fs.ls(foldername)"],"metadata":{}},{"cell_type":"code","source":["try:\n  # TODO: move the file to the azure storage mount point \nexcept :\n  print(\"(file not found -- moved already?)\")\n\n# TODO: List the files in Azure Storage"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Load the dataset into a Spark Dataframe and display it\n\nDon't forget to account for the header field + infer the schema based on the data. Documentation here: https://docs.databricks.com/spark/latest/data-sources/read-csv.html#reading-files\n\nAlso note down the number of jobs that Spark will execute to do this task."],"metadata":{}},{"cell_type":"code","source":["rawGaiaDF = #TODO\ndisplay(rawGaiaDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Print the schema of the raw Dataframe\n\nAs a curiosity, the data model is described here: https://gea.esac.esa.int/archive/documentation/GDR2/Gaia_archive/chap_datamodel/sec_dm_main_tables/ssec_dm_gaia_source.html"],"metadata":{}},{"cell_type":"code","source":["# TODO"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Clean up the dataframe\n\nThe convention for column names in Spark is camel case. Create a new dataframe by renaming the following relevant columns to the right convention:\n- designation\n- ra: renamed to \"rightAscention\"\n- dec: renamed to \"declination\"\n- astrometric_pseudo_colour: renamed to \"starColour\"\n- radial_velocity: renamed to \"radialVelocity\"\n- teff_val: renamed to \"effectiveTemperature\"\n- radius_val: renamed to \"stelarRadius\"\n- lum_val: renamed to \"stelarLuminosity\"\n\n**Note 1**: do this in a single multi-line instruction. After you run the command, see the number of Spark jobs that were created. Now add a display() call to see a preview of the data, and compare the number of Spark jobs created. \n\n** Note 2 **: unlike Dataframes in Numpy or Pandas, Spark dataframes are immutable. That means that even if you think you are updating a value in a Spark Dataframe, you really acting on a copy of it.\n\n** Hint **: you can use the col() Spark function to refer to a column, and alias() to change it's name. The .select() function of the Dataframe allows you to do a query on a dataframe, with it's result being another dataframe. https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/functions.html"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ncleanedUpDF = # TODO\n  \ndisplay(cleanedUpDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Add two calculated columns to the dataframe\n\n- Add a column named \"distance\" which has the mathematical value of **abs(1/parallax)** (1 divided by the value of the parallax column). This will be distances from us to the star in units of _Parsecs_.\n- Add a column named \"magnitude\", with the following mathematical value: **stelarLuminosity + 5 + 5*log(distance)**. Absolute Magnitude is a way to measure how bright a star shines in the sky, at a fixed standard distance. \n\n**Hint:** new columns can be created with the .withColumn() method of a dataframe. More information: https://docs.azuredatabricks.net/spark/1.6/sparkr/functions/withColumn.html#withcolumn"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import log, abs\n\nfullGaiaDF = # TODO"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Data partitioning\n\nAlthough it doesn't look like it, Spark Dataframes are a distributed data structure. The dataframe you've just loaded and processed is actually distributed in the different machines in the cluster. When you run a command, the Spark jobs are being spun up to execute a distributed query, collect the results and present them to you. In the following cell, can you print the number of partitions of the dataframe?\n\n**Hint**: Dataframes are a higher level abstraction over another data structure called RDDs, which have a method to get the number of partitions - https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/rdd/RDD.html#getNumPartitions()"],"metadata":{}},{"cell_type":"code","source":["# TODO"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Register the Dataframe as a table\n\nTo be able to use SQL directly in cells to query a dataframe, you have to register it as a table. Do this with the registerTempTable() method of the dataframe. Name the temporary table \"GaiaDR2\".\n\nMore information here: https://docs.azuredatabricks.net/spark/1.6/sparkr/functions/registerTempTable.html#registertemptable"],"metadata":{}},{"cell_type":"code","source":["# TODO"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Save your processed dataframe as in Apache Parquet format\n\nParquet is a columnar, highly optimized data format with schema support. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. Spark has native support to write dataframes in Parquet.\n\nIn the next cell, write out your processed dataframe as Parquet."],"metadata":{}},{"cell_type":"code","source":["writePath = \"mnt/gaiadr2.parquet\"\n\n# TODO"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%fs ls mnt/"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## Query the table data using SQL\n\nUse the following fields to write SQL queries. Don't use python, rather direct SQL."],"metadata":{}},{"cell_type":"code","source":["%sql\n\n-- todo: what is the minimum distance of a star to earth, and is the star with the largest magnitude?"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql\n\n-- todo: how many stars are closer to us than 1 parsec?"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Native visualizations\n\nYou can integrate visualizations using libraries such as matplotlib. Databricks however also includes native some visualization support. To demonstrate this:\n\na) in the next cell write a SQL query to read the columns \"starColour\" and \"stelarLuminosity\" whe the first value > 1.2 and the second is < 5 .\n\nb) click the icon with types of graphs, under the result of the query, and pick a Scatter plot. Click \"Plot Options\" and make sure that in the Value field you have first the starColour (x-axis) and then the stelarLuminosity (y-axis) fields."],"metadata":{}},{"cell_type":"code","source":["%sql\n\n-- todo\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["#Optional - insert the data into CosmosDb\n\nCosmosDB is Azure's globallt distributed, multimodel, NoSQL repository, and there is a connector from Databricks into/from CosmosDb. In the next cell, write out the contents of the dataframe into a CosmoDB database.\n\nRelevant links:\n\n- How to create a CosmosDB and install the connector: https://codewithsarath.com/querying-cosmos-db-in-azure-databricks-using-cosmos-db-spark-connector/\n- Use the Azure CosmosDb Spark connector: https://docs.databricks.com/spark/latest/data-sources/azure/cosmosdb-connector.html\n- Azure CosmosDb Spark Connector User Guide - https://github.com/Azure/azure-cosmosdb-spark/wiki/Azure-Cosmos-DB-Spark-Connector-User-Guide\n\n**Important**: Remember to delete the CosmosDb Account you created, as costs can be high!"],"metadata":{}},{"cell_type":"code","source":["# TODO"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Congratulations!\n\nIf you made it to here, you've just **[sort of]** found one of the most important correlations of modern stelar physics: there is a a relationship between the colour of the star and how bright it shines. Astrophysics call this the Hertzsprungâ€“Russell diagram. More information can be found here https://en.wikipedia.org/wiki/Hertzsprung%E2%80%93Russell_diagram and some real diagrams from Gaia with the correct maths are here: https://www.cosmos.esa.int/web/gaia/gaiadr2_hrd .\n\n_(disclaimer: this was a very simplified and approximate approach, the calculations actually have many errors!)_"],"metadata":{}}],"metadata":{"name":"Databricks Fundamentals","notebookId":685290754708359},"nbformat":4,"nbformat_minor":0}
